{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install pyautogen \n",
    "! pip install tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.retrieve_utils import TEXT_FORMATS\n",
    "import autogen\n",
    "from autogen.coding import DockerCommandLineCodeExecutor\n",
    "from autogen.agentchat.contrib.agent_builder import AgentBuilder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_api_endpoint = \"http://localhost:11434\" # Development endpoint port forwarded\n",
    "# ollama_api_endpoint = \"http://ollama.ollama.svc.cluster.local:11434\" #Production endpoint for in cluster use\n",
    "\n",
    "# Create an autogen config list\n",
    "config_list = [\n",
    "  {\n",
    "    \"model\": \"llama3.2:latest\",\n",
    "    \"base_url\": ollama_api_endpoint + \"/v1\",\n",
    "    \"api_key\": \"ollama\",\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Working directory coding does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create a docker executor to execute code\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m executor \u001b[38;5;241m=\u001b[39m \u001b[43mDockerCommandLineCodeExecutor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpython:3.12-slim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Execute code using the given docker image name.\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Timeout for each code execution in seconds.\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwork_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoding/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use the temporary directory to store the code files.\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/autogen/coding/docker_commandline_code_executor.py:87\u001b[0m, in \u001b[0;36mDockerCommandLineCodeExecutor.__init__\u001b[0;34m(self, image, container_name, timeout, work_dir, auto_remove, stop_container)\u001b[0m\n\u001b[1;32m     84\u001b[0m     work_dir \u001b[38;5;241m=\u001b[39m Path(work_dir)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m work_dir\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorking directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwork_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m client \u001b[38;5;241m=\u001b[39m docker\u001b[38;5;241m.\u001b[39mfrom_env()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Check if the image exists\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Working directory coding does not exist."
     ]
    }
   ],
   "source": [
    "# create a docker executor to execute code\n",
    "executor = DockerCommandLineCodeExecutor(\n",
    "    image=\"python:3.12-slim\",  # Execute code using the given docker image name.\n",
    "    timeout=30,  # Timeout for each code execution in seconds.\n",
    "    work_dir=\"coding/\",  # Use the temporary directory to store the code files.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AssistantAgent named \"assistant\"\n",
    "assistant = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config={\n",
    "        \"cache_seed\": 41,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0,  # temperature for sampling\n",
    "    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    ")\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\"executor\": executor},  # Use the docker command line code executor.\n",
    ")\n",
    "# the assistant receives a message from the user_proxy, which contains the task description\n",
    "chat_res = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"\"\"\n",
    "    Write me a simple Flask application which serves a static HTML app.\n",
    "    The app should have a single route that serves the HTML file.\n",
    "    The HTML file should have a button that when clicked, sends a POST request to the server (to http://ollama.ollama.svc.cluster.local:11434). \n",
    "    The server is an Ollama endpoint which uses an OpenAI API compatible model to generate a response.\n",
    "    \"\"\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
